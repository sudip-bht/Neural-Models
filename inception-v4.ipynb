{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:32:38.620380Z","iopub.status.busy":"2024-05-23T16:32:38.620100Z","iopub.status.idle":"2024-05-23T16:33:01.298189Z","shell.execute_reply":"2024-05-23T16:33:01.297381Z","shell.execute_reply.started":"2024-05-23T16:32:38.620355Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: timm in ./.venv/lib/python3.12/site-packages (1.0.3)\n","Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from timm) (2.3.0)\n","Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (from timm) (0.18.0)\n","Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (from timm) (0.23.0)\n","Requirement already satisfied: safetensors in ./.venv/lib/python3.12/site-packages (from timm) (0.4.3)\n","Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm) (2024.3.1)\n","Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm) (24.0)\n","Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->timm) (4.11.0)\n","Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->timm) (3.3)\n","Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->timm) (3.1.4)\n","Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision->timm) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision->timm) (10.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["/Users/sudipbhattarai/Desktop/Projects/Neural-Models/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["%pip install timm\n","import torch\n","import timm\n","import os\n","import torchvision\n","from torchvision import transforms\n","from torchvision.transforms.functional import InterpolationMode\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score,ConfusionMatrixDisplay\n","import seaborn as sns\n","import sys\n","sys.path.append('../input/pytorchimagemodels')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:01.300258Z","iopub.status.busy":"2024-05-23T16:33:01.299810Z","iopub.status.idle":"2024-05-23T16:33:01.351761Z","shell.execute_reply":"2024-05-23T16:33:01.350655Z","shell.execute_reply.started":"2024-05-23T16:33:01.300223Z"},"trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","data_path = \"/kaggle/input/ham10001/Data folder\"\n","train_crop_size = 299\n","interpolation = \"bilinear\"\n","val_crop_size = 299\n","val_resize_size = 299\n","model_name = \"inception_v3\"\n","pretrained = True\n","batch_size = 32\n","num_workers = 4\n","learning_rate = 0.001\n","momentum = 0.9\n","weight_decay = 1e-4\n","lr_step_size = 30\n","lr_gamma = 0.001\n","epochs = 100\n","train_dir = os.path.join(data_path,\"train\")\n","val_dir = os.path.join(data_path, \"val\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:01.353375Z","iopub.status.busy":"2024-05-23T16:33:01.353033Z","iopub.status.idle":"2024-05-23T16:33:12.817187Z","shell.execute_reply":"2024-05-23T16:33:12.816285Z","shell.execute_reply.started":"2024-05-23T16:33:01.353350Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/ham10001/Data folder/train'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m interpolation \u001b[38;5;241m=\u001b[39m InterpolationMode(interpolation)\n\u001b[1;32m      3\u001b[0m TRAIN_TRANSFORM_IMG \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(train_crop_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                         std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m] )\n\u001b[1;32m     10\u001b[0m ])\n\u001b[0;32m---> 13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_TRANSFORM_IMG\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m TEST_TRANSFORM_IMG \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     18\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(val_resize_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation),\n\u001b[1;32m     19\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(val_crop_size),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m                         std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m] )\n\u001b[1;32m     24\u001b[0m     ])\n\u001b[1;32m     26\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mImageFolder(\n\u001b[1;32m     27\u001b[0m     val_dir,\n\u001b[1;32m     28\u001b[0m     transform\u001b[38;5;241m=\u001b[39mTEST_TRANSFORM_IMG\n\u001b[1;32m     29\u001b[0m )\n","File \u001b[0;32m~/Desktop/Projects/Neural-Models/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n","File \u001b[0;32m~/Desktop/Projects/Neural-Models/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n","File \u001b[0;32m~/Desktop/Projects/Neural-Models/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Projects/Neural-Models/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/ham10001/Data folder/train'"]}],"source":["interpolation = InterpolationMode(interpolation)\n","\n","TRAIN_TRANSFORM_IMG = transforms.Compose([\n","\n","transforms.RandomResizedCrop(train_crop_size, interpolation=interpolation),\n","    transforms.PILToTensor(),\n","    transforms.ConvertImageDtype(torch.float),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                        std=[0.229, 0.224, 0.225] )\n","])\n","\n","\n","dataset =torchvision.datasets.ImageFolder(\n","    train_dir,\n","    transform=TRAIN_TRANSFORM_IMG\n",")\n","TEST_TRANSFORM_IMG = transforms.Compose([\n","    transforms.Resize(val_resize_size, interpolation=interpolation),\n","    transforms.CenterCrop(val_crop_size),\n","    transforms.PILToTensor(),\n","    transforms.ConvertImageDtype(torch.float),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                        std=[0.229, 0.224, 0.225] )\n","    ])\n","\n","dataset_test = torchvision.datasets.ImageFolder(\n","    val_dir,\n","    transform=TEST_TRANSFORM_IMG\n",")\n","\n","print(\"Creating data loaders\")\n","train_sampler = torch.utils.data.RandomSampler(dataset)\n","test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n","\n","data_loader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=batch_size,\n","    sampler=train_sampler,\n","    num_workers=num_workers,\n","    pin_memory=True\n",")\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers, pin_memory=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:12.818535Z","iopub.status.busy":"2024-05-23T16:33:12.818247Z","iopub.status.idle":"2024-05-23T16:33:14.693796Z","shell.execute_reply":"2024-05-23T16:33:14.693004Z","shell.execute_reply.started":"2024-05-23T16:33:12.818511Z"},"trusted":true},"outputs":[],"source":["print(\"Creating model\")\n","print(\"Num classes = \", len(dataset.classes))\n","model = timm.create_model('inception_v4', pretrained=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_filenames_train=[sample[0] for sample in dataset.samples]\n","image_filenames_test=[sample[0] for sample in dataset_test.samples]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:14.697093Z","iopub.status.busy":"2024-05-23T16:33:14.696728Z","iopub.status.idle":"2024-05-23T16:33:14.702240Z","shell.execute_reply":"2024-05-23T16:33:14.700859Z","shell.execute_reply.started":"2024-05-23T16:33:14.697065Z"},"trusted":true},"outputs":[],"source":["model.aux_logits = False\n","model.AuxLogits = None"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:14.703876Z","iopub.status.busy":"2024-05-23T16:33:14.703613Z","iopub.status.idle":"2024-05-23T16:33:15.000613Z","shell.execute_reply":"2024-05-23T16:33:14.999730Z","shell.execute_reply.started":"2024-05-23T16:33:14.703854Z"},"trusted":true},"outputs":[],"source":["model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:15.002498Z","iopub.status.busy":"2024-05-23T16:33:15.002073Z","iopub.status.idle":"2024-05-23T16:33:15.008195Z","shell.execute_reply":"2024-05-23T16:33:15.007249Z","shell.execute_reply.started":"2024-05-23T16:33:15.002447Z"},"trusted":true},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:15.009487Z","iopub.status.busy":"2024-05-23T16:33:15.009201Z","iopub.status.idle":"2024-05-23T16:33:15.020067Z","shell.execute_reply":"2024-05-23T16:33:15.019337Z","shell.execute_reply.started":"2024-05-23T16:33:15.009438Z"},"trusted":true},"outputs":[],"source":["optimizer = torch.optim.SGD(\n","    model.parameters(),\n","    lr=learning_rate,\n","    momentum=momentum,\n","    weight_decay=weight_decay,\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:15.021409Z","iopub.status.busy":"2024-05-23T16:33:15.021122Z","iopub.status.idle":"2024-05-23T16:33:15.037493Z","shell.execute_reply":"2024-05-23T16:33:15.036785Z","shell.execute_reply.started":"2024-05-23T16:33:15.021384Z"},"trusted":true},"outputs":[],"source":["n_epochs = 10\n","early_stopping_tolerance = 3\n","early_stopping_threshold = 0.03\n","early_stopping_counter = 0\n","best_loss = float('inf')\n","best_model_wts = model.state_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:15.039551Z","iopub.status.busy":"2024-05-23T16:33:15.038854Z","iopub.status.idle":"2024-05-23T16:33:15.043903Z","shell.execute_reply":"2024-05-23T16:33:15.042928Z","shell.execute_reply.started":"2024-05-23T16:33:15.039519Z"},"trusted":true},"outputs":[],"source":["lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T16:33:15.045193Z","iopub.status.busy":"2024-05-23T16:33:15.044933Z"},"trusted":true},"outputs":[],"source":["train_loss_history = []\n","epoch_train_losses = []\n","epoch_accuracies = []\n","epoch_precisions = []\n","epoch_recalls = []\n","epoch_f1_scores = []\n","print(\"Start training\")\n","for epoch in range(epochs):\n","        model.train()\n","        epoch_loss = 0\n","        len_dataset = 0\n","        for step, (image, target) in enumerate(data_loader):\n","            image, target = image.to(device), target.to(device)\n","            output = model(image)\n","            loss = criterion(output, target)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += output.shape[0] * loss.item()\n","            len_dataset += output.shape[0];\n","            if step % 10 == 0:\n","                print('Epoch: ', epoch, '| step : %d' % step, '| train loss : %0.4f' % loss.item() )\n","        epoch_loss = epoch_loss / len_dataset\n","        epoch_train_losses.append(epoch_loss)\n","        print('Epoch: ', epoch, '| train loss :  %0.4f' % epoch_loss )\n","        print('Lenght Data Set :  %0.4f' % len_dataset )\n","        # Early stopping\n","        if epoch>29:\n","          torch.save(model.state_dict(), '/kaggle/working/inceptionv4.pt')\n","        if epoch_loss < best_loss:\n","            best_loss = epoch_loss\n","            best_model_wts = model.state_dict()\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","\n","        if early_stopping_counter >= early_stopping_tolerance or best_loss <= early_stopping_threshold:\n","            print(\"\\nTerminating: early stopping\")\n","            epochs=epoch\n","            break\n","        lr_scheduler.step()\n","\n","model.load_state_dict(best_model_wts)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), '/kaggle/working/inceptionv4.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["epoch_test_loss_intrain=[]\n","misclassified_images_train = []\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Iterate over epochs\n","for epoch in range(epochs):\n","    predicted_labels_train = []\n","    ground_truth_labels_train = []\n","    with torch.no_grad():  # No gradient calculation during evaluation\n","        running_loss = 0\n","        # Iterate over the training data loader\n","        for step, (image, target) in enumerate(data_loader):\n","            image, target = image.to(device), target.to(device)\n","            output = model(image)\n","            _, predicted = torch.max(output, 1)  # Get the predicted labels\n","            predicted_labels_train.extend(predicted.cpu().numpy())  # Convert to numpy array and add to predicted labels list\n","            ground_truth_labels_train.extend(target.cpu().numpy())\n","            loss = criterion(output, target)\n","            running_loss += loss.item()\n","        running_loss /= len(data_loader)\n","        epoch_test_loss_intrain.append(running_loss)\n","        print(f'Epoch {epoch + 1} test loss: {running_loss:.4f}')\n","\n","        # Identify misclassified images\n","        misclassified_indices = [i for i, (true, pred) in enumerate(zip(ground_truth_labels_train, predicted_labels_train)) if true != pred]\n","        misclassified_images_epoch = [image_filenames_train[i] for i in misclassified_indices]\n","        misclassified_images_train.extend(misclassified_images_epoch)\n","\n","# Save misclassified images to a .txt file\n","misclassified_file_train = 'misclassified_images_train.txt'\n","with open(misclassified_file_train, 'w') as f:\n","    for img in misclassified_images_train:\n","        f.write(f\"{img}\\n\")\n","\n","print(f\"Misclassified images during training saved to {misclassified_file_train}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["epoch_test_loss_intest=[]\n","misclassified_images = []\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Iterate over epochs\n","for epoch in range(epochs):\n","    predicted_labels_test = []\n","    ground_truth_labels_test = []\n","    with torch.no_grad():  # No gradient calculation during evaluation\n","        running_loss = 0\n","        # Iterate over the test data loader\n","        for step, (image, target) in enumerate(data_loader_test):\n","            image, target = image.to(device), target.to(device)\n","            output = model(image)\n","            _, predicted = torch.max(output, 1)  # Get the predicted labels\n","            predicted_labels_test.extend(predicted.cpu().numpy())  # Convert to numpy array and add to predicted labels list\n","            ground_truth_labels_test.extend(target.cpu().numpy())\n","            loss = criterion(output, target)\n","            running_loss += loss.item()\n","        running_loss /= len(data_loader_test)\n","        epoch_test_loss_intest.append(running_loss)\n","        print(f'Epoch {epoch + 1} test loss: {running_loss:.4f}')\n","\n","        # Identify misclassified images\n","        misclassified_indices = [i for i, (true, pred) in enumerate(zip(ground_truth_labels_test, predicted_labels_test)) if true != pred]\n","        misclassified_images_epoch = [image_filenames_test[i] for i in misclassified_indices]\n","        misclassified_images.extend(misclassified_images_epoch)\n","\n","# Save misclassified images to a .txt file\n","misclassified_file = 'misclassified_images_test.txt'\n","with open(misclassified_file, 'w') as f:\n","    for img in misclassified_images:\n","        f.write(f\"{img}\\n\")\n","\n","print(f\"Misclassified images saved to {misclassified_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm_train= confusion_matrix(ground_truth_labels_train,predicted_labels_train)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=[\"Benign\", \"Malignant\"])\n","disp.plot()\n","plt.show()\n","TP = cm_train[1, 1]\n","TN = cm_train[0, 0]\n","FP = cm_train[0, 1]\n","FN = cm_train[1, 0]\n","accuracy = (TP + TN) / float(TP + TN + FP + FN)\n","precision = TP / float(TP + FP)\n","recall = TP / float(TP + FN)\n","f1 = 2 * (precision * recall) / (precision + recall)\n","sensitivity = recall  # Sensitivity is the same as recall\n","specificity = TN / float(TN + FP)\n","print(f'Precision train: {precision:.2f}')\n","print(f'Recall train: {recall:.2f}')\n","print(f'F1 Score train: {f1:.2f}')\n","print(f'Accuracy train: {accuracy:.2f}')\n","print(f'Sensitivity:{sensitivity:2f}')\n","print(f'Specificity:{specificity:2f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm_test= confusion_matrix(ground_truth_labels_test,predicted_labels_test)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=[\"Benign\", \"Malignant\"])\n","disp.plot()\n","plt.show()\n","TP = cm_test[1, 1]\n","TN = cm_test[0, 0]\n","FP = cm_test[0, 1]\n","FN = cm_test[1, 0]\n","accuracy = (TP + TN) / float(TP + TN + FP + FN)\n","precision = TP / float(TP + FP)\n","recall = TP / float(TP + FN)\n","f1 = 2 * (precision * recall) / (precision + recall)\n","sensitivity = recall  # Sensitivity is the same as recall\n","specificity = TN / float(TN + FP)\n","print(f'Precision Test: {precision:.2f}')\n","print(f'Recall Test: {recall:.2f}')\n","print(f'F1 Score Test: {f1:.2f}')\n","print(f'Accuracy Test: {accuracy:.2f}')\n","print(f'Sensitivity Test:{sensitivity:2f}')\n","print(f'Specificity Test:{specificity:2f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(12, 5))\n","plt.plot(range(1, len(epoch_train_losses) + 1), epoch_train_losses, label='Training Loss')\n","plt.plot(range(1, len(epoch_test_loss_intrain) + 1), epoch_test_loss_intrain, label='Test Loss in Train')\n","plt.plot(range(1, len(epoch_test_loss_intest) + 1), epoch_test_loss_intest, label='Test Loss in Test')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5071655,"sourceId":8498901,"sourceType":"datasetVersion"},{"datasetId":5130353,"sourceId":8578869,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
